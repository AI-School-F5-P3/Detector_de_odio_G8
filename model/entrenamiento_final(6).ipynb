{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsNationalist</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\nDont you reckon them 'black lives matter' ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are a very large number of people who do...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Arab dude is absolutely right, he should h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  IsToxic  IsAbusive  \\\n",
       "0  If only people would just take a step back and...    False      False   \n",
       "1  Law enforcement is not trained to shoot to app...     True       True   \n",
       "2  \\r\\nDont you reckon them 'black lives matter' ...     True       True   \n",
       "3  There are a very large number of people who do...    False      False   \n",
       "4  The Arab dude is absolutely right, he should h...    False      False   \n",
       "\n",
       "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
       "0     False          False      False         False     False          False   \n",
       "1     False          False      False         False     False          False   \n",
       "2     False          False       True         False     False          False   \n",
       "3     False          False      False         False     False          False   \n",
       "4     False          False      False         False     False          False   \n",
       "\n",
       "   IsReligiousHate  \n",
       "0            False  \n",
       "1            False  \n",
       "2            False  \n",
       "3            False  \n",
       "4            False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Información del dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Text             1000 non-null   object\n",
      " 1   IsToxic          1000 non-null   bool  \n",
      " 2   IsAbusive        1000 non-null   bool  \n",
      " 3   IsThreat         1000 non-null   bool  \n",
      " 4   IsProvocative    1000 non-null   bool  \n",
      " 5   IsObscene        1000 non-null   bool  \n",
      " 6   IsHatespeech     1000 non-null   bool  \n",
      " 7   IsRacist         1000 non-null   bool  \n",
      " 8   IsNationalist    1000 non-null   bool  \n",
      " 9   IsReligiousHate  1000 non-null   bool  \n",
      "dtypes: bool(9), object(1)\n",
      "memory usage: 16.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('youtoxic_english_1000.csv')\n",
    "\n",
    "# Eliminar las columnas 'CommentId' y 'VideoId'\n",
    "df = df.drop(columns=['CommentId', 'VideoId', 'IsSexist', 'IsHomophobic', 'IsRadicalism'])\n",
    "\n",
    "# Mostramos las primeras filas y la información del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())\n",
    "print(\"\\nInformación del dataset:\")\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos por columna antes de la limpieza:\n",
      "Text               0\n",
      "IsToxic            0\n",
      "IsAbusive          0\n",
      "IsThreat           0\n",
      "IsProvocative      0\n",
      "IsObscene          0\n",
      "IsHatespeech       0\n",
      "IsRacist           0\n",
      "IsNationalist      0\n",
      "IsReligiousHate    0\n",
      "dtype: int64\n",
      "\n",
      "Total de filas antes de la limpieza: 1000\n",
      "\n",
      "Estadísticas finales:\n",
      "--------------------------------------------------\n",
      "Distribución de mensajes dañinos vs no dañinos:\n",
      "is_harmful\n",
      "0    0.538\n",
      "1    0.462\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribución por tipo de contenido dañino:\n",
      "IsToxic: 462 casos (46.20%)\n",
      "IsAbusive: 353 casos (35.30%)\n",
      "IsThreat: 21 casos (2.10%)\n",
      "IsProvocative: 161 casos (16.10%)\n",
      "IsObscene: 100 casos (10.00%)\n",
      "IsHatespeech: 138 casos (13.80%)\n",
      "IsRacist: 125 casos (12.50%)\n",
      "IsNationalist: 8 casos (0.80%)\n",
      "IsReligiousHate: 12 casos (1.20%)\n"
     ]
    }
   ],
   "source": [
    "# [Celda 3] - Preparación y limpieza de datos\n",
    "\n",
    "# Primero mostramos información sobre valores nulos en el dataset\n",
    "print(\"Valores nulos por columna antes de la limpieza:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nTotal de filas antes de la limpieza:\", len(df))\n",
    "\n",
    "# Definimos las columnas que indican contenido dañino\n",
    "toxic_columns = ['IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', \n",
    "                'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist',\n",
    "                'IsReligiousHate']\n",
    "\n",
    "# Convertir los valores \"TRUE\"/\"FALSE\" a 1/0 en las columnas tóxicas (si es necesario)\n",
    "df[toxic_columns] = df[toxic_columns].replace({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "# Creamos una nueva columna que será 1 si cualquiera de las categorías es 1\n",
    "df['is_harmful'] = df[toxic_columns].any(axis=1).astype(int)\n",
    "\n",
    "# Preparamos X (textos) e y (etiquetas)\n",
    "X = df['Text']\n",
    "y = df['is_harmful']\n",
    "\n",
    "# Mostramos estadísticas finales\n",
    "print(\"\\nEstadísticas finales:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Distribución de mensajes dañinos vs no dañinos:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "print(\"\\nDistribución por tipo de contenido dañino:\")\n",
    "for col in toxic_columns:\n",
    "    positivos = df[col].sum()\n",
    "    porcentaje = (positivos / len(df)) * 100\n",
    "    print(f\"{col}: {positivos} casos ({porcentaje:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 850\n",
      "Tamaño del conjunto de prueba: 150\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y  # Mantenemos la proporción de clases\n",
    ")\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(X_train))\n",
    "print(\"Tamaño del conjunto de prueba:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la matriz de características de entrenamiento: (850, 1566)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_1 = TfidfVectorizer(\n",
    "    max_features=5000,  # Limitamos el número de características\n",
    "    min_df=2,          # Ignoramos términos que aparecen en menos de 2 documentos\n",
    "    stop_words='english'  # Removemos stopwords en inglés\n",
    ")\n",
    "\n",
    "# Transformamos los textos\n",
    "X_train_vectorized_1 = vectorizer_1.fit_transform(X_train)\n",
    "X_test_vectorized_1 = vectorizer_1.transform(X_test)\n",
    "\n",
    "print(\"Dimensiones de la matriz de características de entrenamiento:\", X_train_vectorized_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo final entrenado con los mejores hiperparámetros!\n"
     ]
    }
   ],
   "source": [
    "final_svm_model = SVC(\n",
    "    C=109.42844096035671,\n",
    "    kernel='rbf',\n",
    "    gamma=1.1662316858478285,\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    "    class_weight=None,\n",
    "    tol=0.00043366566542372227,\n",
    "    shrinking=False,\n",
    "    max_iter=1697\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento completo\n",
    "final_svm_model.fit(X_train_vectorized_1, y_train)\n",
    "\n",
    "print(\"Modelo final entrenado con los mejores hiperparámetros!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-12 14:45:48,681] A new study created in memory with name: no-name-40dd0288-30f4-4a70-9a31-5cdecdf6b894\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,701] Trial 0 finished with value: 0.6635436885150716 and parameters: {'alpha': 0.09347877541058479, 'fit_prior': False}. Best is trial 0 with value: 0.6635436885150716.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,723] Trial 1 finished with value: 0.6447021350719156 and parameters: {'alpha': 0.0020115201329949576, 'fit_prior': False}. Best is trial 0 with value: 0.6635436885150716.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,748] Trial 2 finished with value: 0.6611755337679789 and parameters: {'alpha': 0.14962344236534747, 'fit_prior': True}. Best is trial 0 with value: 0.6635436885150716.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,773] Trial 3 finished with value: 0.6494094128966971 and parameters: {'alpha': 0.002940630032054419, 'fit_prior': False}. Best is trial 0 with value: 0.6635436885150716.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,797] Trial 4 finished with value: 0.6717720931004164 and parameters: {'alpha': 47.29705506792075, 'fit_prior': False}. Best is trial 4 with value: 0.6717720931004164.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,818] Trial 5 finished with value: 0.6364695831052273 and parameters: {'alpha': 2.9117688928190702e-05, 'fit_prior': False}. Best is trial 4 with value: 0.6717720931004164.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,841] Trial 6 finished with value: 0.643520131389041 and parameters: {'alpha': 8.435898988665078e-05, 'fit_prior': True}. Best is trial 4 with value: 0.6717720931004164.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,861] Trial 7 finished with value: 0.6576378174820419 and parameters: {'alpha': 0.025368013469065438, 'fit_prior': True}. Best is trial 4 with value: 0.6717720931004164.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,883] Trial 8 finished with value: 0.6364695831052273 and parameters: {'alpha': 2.7677270410419316e-05, 'fit_prior': False}. Best is trial 4 with value: 0.6717720931004164.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,914] Trial 9 finished with value: 0.6706274158496326 and parameters: {'alpha': 0.37557820287131555, 'fit_prior': False}. Best is trial 4 with value: 0.6717720931004164.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,952] Trial 10 finished with value: 0.5376457804542213 and parameters: {'alpha': 79.91487636139567, 'fit_prior': True}. Best is trial 4 with value: 0.6717720931004164.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:48,978] Trial 11 finished with value: 0.6729416546392608 and parameters: {'alpha': 21.917394138421812, 'fit_prior': False}. Best is trial 11 with value: 0.6729416546392608.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,010] Trial 12 finished with value: 0.6729499494019476 and parameters: {'alpha': 39.97022498991707, 'fit_prior': False}. Best is trial 12 with value: 0.6729499494019476.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,040] Trial 13 finished with value: 0.6764835183065413 and parameters: {'alpha': 4.706261500090809, 'fit_prior': False}. Best is trial 13 with value: 0.6764835183065413.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,095] Trial 14 finished with value: 0.6776655219894159 and parameters: {'alpha': 3.811262030596834, 'fit_prior': False}. Best is trial 14 with value: 0.6776655219894159.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,148] Trial 15 finished with value: 0.6729706863086647 and parameters: {'alpha': 1.2018793524756828, 'fit_prior': False}. Best is trial 14 with value: 0.6776655219894159.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,184] Trial 16 finished with value: 0.6788392309096037 and parameters: {'alpha': 3.0550921683930063, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,242] Trial 17 finished with value: 0.6788392309096037 and parameters: {'alpha': 3.040790731460047, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,281] Trial 18 finished with value: 0.6541042485774482 and parameters: {'alpha': 0.015796018231707943, 'fit_prior': True}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,314] Trial 19 finished with value: 0.6764752235438545 and parameters: {'alpha': 5.3692327703329426, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,344] Trial 20 finished with value: 0.6729748336900081 and parameters: {'alpha': 0.9508965784641206, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,375] Trial 21 finished with value: 0.6764752235438545 and parameters: {'alpha': 5.683549560866072, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,401] Trial 22 finished with value: 0.6694454121667578 and parameters: {'alpha': 1.2837127663057648, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,432] Trial 23 finished with value: 0.674119510940792 and parameters: {'alpha': 12.284670982751866, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,464] Trial 24 finished with value: 0.676491813069228 and parameters: {'alpha': 2.2063263533545157, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,495] Trial 25 finished with value: 0.6635519832777583 and parameters: {'alpha': 0.22095574874159082, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,521] Trial 26 finished with value: 0.6741236583221354 and parameters: {'alpha': 0.4922775332309535, 'fit_prior': True}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,549] Trial 27 finished with value: 0.6635395411337283 and parameters: {'alpha': 0.06284446118423431, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,573] Trial 28 finished with value: 0.674119510940792 and parameters: {'alpha': 11.730041013905879, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,597] Trial 29 finished with value: 0.6623658322135404 and parameters: {'alpha': 0.07949733513205066, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,625] Trial 30 finished with value: 0.6505914165795716 and parameters: {'alpha': 0.011245858315877866, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,651] Trial 31 finished with value: 0.676491813069228 and parameters: {'alpha': 2.3158696221301356, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,678] Trial 32 finished with value: 0.6788350835282603 and parameters: {'alpha': 2.73866957455279, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,711] Trial 33 finished with value: 0.674119510940792 and parameters: {'alpha': 15.161556075718092, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,743] Trial 34 finished with value: 0.672983128452695 and parameters: {'alpha': 0.447171725878125, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,782] Trial 35 finished with value: 0.6399990046284776 and parameters: {'alpha': 0.0005402895752752813, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,815] Trial 36 finished with value: 0.6694205278786974 and parameters: {'alpha': 94.80977653994745, 'fit_prior': False}. Best is trial 16 with value: 0.6788392309096037.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,854] Trial 37 finished with value: 0.6870717828762919 and parameters: {'alpha': 3.3694442054186435, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,894] Trial 38 finished with value: 0.6623533900695101 and parameters: {'alpha': 0.1463542442628528, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,924] Trial 39 finished with value: 0.6423381277061663 and parameters: {'alpha': 0.0025262902411705766, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,966] Trial 40 finished with value: 0.5529413228487533 and parameters: {'alpha': 28.022285558645848, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:49,999] Trial 41 finished with value: 0.6847202176545729 and parameters: {'alpha': 2.9605147093993045, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:50,035] Trial 42 finished with value: 0.6764876656878847 and parameters: {'alpha': 0.7489766478062714, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:50,073] Trial 43 finished with value: 0.6835423613530417 and parameters: {'alpha': 2.5595814879135705, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:50,103] Trial 44 finished with value: 0.6094162146021002 and parameters: {'alpha': 9.807088421699207, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:50,133] Trial 45 finished with value: 0.6670565205129483 and parameters: {'alpha': 0.2213077090463083, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:50,165] Trial 46 finished with value: 0.6470495529122912 and parameters: {'alpha': 7.313546755489767, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:50,199] Trial 47 finished with value: 0.6870717828762919 and parameters: {'alpha': 2.0237487624066453, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:50,242] Trial 48 finished with value: 0.5435267671991905 and parameters: {'alpha': 45.29869350101766, 'fit_prior': True}. Best is trial 37 with value: 0.6870717828762919.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9652\\4272701593.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-12 14:45:50,285] Trial 49 finished with value: 0.6929569170026045 and parameters: {'alpha': 1.664189892739271, 'fit_prior': True}. Best is trial 49 with value: 0.6929569170026045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros para MultinomialNB:\n",
      "{'alpha': 1.664189892739271, 'fit_prior': True}\n",
      "Mejor precisión: 0.6929569170026045\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir la función objetivo para Optuna con MultinomialNB\n",
    "def objective_nb(trial):\n",
    "    # Definir los hiperparámetros a optimizar\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
    "    fit_prior = trial.suggest_categorical('fit_prior', [True, False])  # Ajustar o no las probabilidades previas\n",
    "    \n",
    "    # Crear el modelo Naive Bayes con los hiperparámetros seleccionados\n",
    "    nb_model = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n",
    "    \n",
    "    # Evaluar el modelo con validación cruzada\n",
    "    scores = cross_val_score(nb_model, X_train_vectorized_1, y_train, cv=3, scoring='accuracy')\n",
    "    accuracy = scores.mean()  # Promedio de accuracy\n",
    "    \n",
    "    return accuracy  # Optuna maximiza este valor\n",
    "\n",
    "# Crear un estudio de optimización para MultinomialNB\n",
    "study_nb = optuna.create_study(direction='maximize')\n",
    "study_nb.optimize(objective_nb, n_trials=50)  # Ajusta el número de pruebas según el tiempo disponible\n",
    "\n",
    "# Resultados de la mejor prueba para MultinomialNB\n",
    "print(\"Mejores hiperparámetros para MultinomialNB:\")\n",
    "print(study_nb.best_trial.params)\n",
    "print(\"Mejor precisión:\", study_nb.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en conjunto de prueba para MultinomialNB: 0.72\n",
      "Reporte de clasificación para MultinomialNB:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.77        81\n",
      "           1       0.76      0.57      0.65        69\n",
      "\n",
      "    accuracy                           0.72       150\n",
      "   macro avg       0.73      0.71      0.71       150\n",
      "weighted avg       0.73      0.72      0.71       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraer los mejores hiperparámetros del estudio de MultinomialNB\n",
    "best_params_nb = study_nb.best_trial.params\n",
    "\n",
    "# Crear el modelo final con los mejores hiperparámetros\n",
    "final_nb_model = MultinomialNB(\n",
    "    alpha=best_params_nb['alpha'],\n",
    "    fit_prior=best_params_nb['fit_prior']\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento completo\n",
    "final_nb_model.fit(X_train_vectorized_1, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_nb = final_nb_model.predict(X_test_vectorized_1)\n",
    "\n",
    "# Calcular y mostrar la precisión y el reporte de clasificación\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "report_nb = classification_report(y_test, y_pred_nb)\n",
    "\n",
    "print(\"Accuracy en conjunto de prueba para MultinomialNB:\", accuracy_nb)\n",
    "print(\"Reporte de clasificación para MultinomialNB:\\n\", report_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en conjunto de prueba para RandomForestClassifier: 0.7066666666666667\n",
      "Reporte de clasificación para RandomForestClassifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.91      0.77        81\n",
      "           1       0.82      0.46      0.59        69\n",
      "\n",
      "    accuracy                           0.71       150\n",
      "   macro avg       0.74      0.69      0.68       150\n",
      "weighted avg       0.74      0.71      0.69       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "final_rf_model = RandomForestClassifier(\n",
    "    n_estimators=235,\n",
    "    max_depth=44,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento completo\n",
    "final_rf_model.fit(X_train_vectorized_1, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_rf = final_rf_model.predict(X_test_vectorized_1)\n",
    "\n",
    "# Calcular y mostrar la precisión y el reporte de clasificación\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Accuracy en conjunto de prueba para RandomForestClassifier:\", accuracy_rf)\n",
    "print(\"Reporte de clasificación para RandomForestClassifier:\\n\", report_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones después de la reducción:\n",
      "Entrenamiento: (891, 1000)\n",
      "Validación: (99, 1000)\n",
      "Prueba: (10, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1, Accuracy en validación: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 2, Accuracy en validación: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 3, Accuracy en validación: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1697).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 4, Accuracy en validación: 0.7778\n",
      "Early stopping activado en época 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# 1. Primero dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.01,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Separamos el conjunto temporal en entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# 2. Vectorizamos los datos\n",
    "vectorizer_2 = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "# Vectorizamos cada conjunto por separado\n",
    "X_train_vectorized_2 = vectorizer_2.fit_transform(X_train)\n",
    "X_val_vectorized = vectorizer_2.transform(X_val)\n",
    "X_test_vectorized_2 = vectorizer_2.transform(X_test)\n",
    "\n",
    "# Reducir dimensionalidad\n",
    "selector = SelectKBest(score_func=chi2, k=1000)\n",
    "X_train_reduced = selector.fit_transform(X_train_vectorized_2, y_train)\n",
    "X_val_reduced = selector.transform(X_val_vectorized)\n",
    "X_test_reduced = selector.transform(X_test_vectorized_2)\n",
    "\n",
    "print(\"Dimensiones después de la reducción:\")\n",
    "print(\"Entrenamiento:\", X_train_reduced.shape)\n",
    "print(\"Validación:\", X_val_reduced.shape)\n",
    "print(\"Prueba:\", X_test_reduced.shape)\n",
    "\n",
    "# Integración de MLflow\n",
    "import mlflow\n",
    "mlflow.set_experiment(\"my-experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\n",
    "        \"max_depth\": 5,\n",
    "        \"n_estimators\": 100,\n",
    "        \"learning_rate\": 0.1\n",
    "    })\n",
    "\n",
    "    # 4. Creamos y entrenamos el ensemble con bagging\n",
    "    bagged_ensemble = BaggingClassifier(\n",
    "        estimator=VotingClassifier(\n",
    "            estimators=[\n",
    "                ('random_forest', final_rf_model),\n",
    "                ('naive_bayes', final_nb_model),\n",
    "                ('svm', final_svm_model)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        ),\n",
    "        n_estimators=10,\n",
    "        max_samples=0.8,\n",
    "        max_features=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 5. Implementamos early stopping\n",
    "    class EarlyStoppingMonitor:\n",
    "        def __init__(self, patience=3, min_delta=0.001):\n",
    "            self.patience = patience\n",
    "            self.min_delta = min_delta\n",
    "            self.best_score = None\n",
    "            self.counter = 0\n",
    "            self.best_model = None\n",
    "            \n",
    "        def check(self, model, val_score):\n",
    "            if self.best_score is None or val_score > self.best_score + self.min_delta:\n",
    "                self.best_score = val_score\n",
    "                self.counter = 0\n",
    "                self.best_model = model\n",
    "                return False\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                return self.counter >= self.patience\n",
    "\n",
    "    # Entrenamiento con early stopping\n",
    "    monitor = EarlyStoppingMonitor(patience=3)\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # Entrenar el modelo\n",
    "        bagged_ensemble.fit(X_train_reduced, y_train)\n",
    "        \n",
    "        # Evaluar en el conjunto de validación\n",
    "        val_accuracy = bagged_ensemble.score(X_val_reduced, y_val)\n",
    "        print(f\"Época {epoch + 1}, Accuracy en validación: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if monitor.check(bagged_ensemble, val_accuracy):\n",
    "            print(\"Early stopping activado en época\", epoch + 1)\n",
    "            break\n",
    "\n",
    "    # Usar el mejor modelo encontrado\n",
    "    best_model = monitor.best_model if monitor.best_model is not None else bagged_ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados finales:\n",
      "Accuracy en entrenamiento (threshold=0.59): 0.8013\n",
      "Accuracy en prueba (threshold=0.59): 0.8000\n",
      "Porcentaje de overfitting: 0.17%\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83         5\n",
      "           1       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.86      0.80      0.79        10\n",
      "weighted avg       0.86      0.80      0.79        10\n",
      "\n",
      "\n",
      "Top 10 características más importantes:\n",
      "run: 8.1457\n",
      "fuck: 7.1270\n",
      "over: 6.6136\n",
      "them: 5.8197\n",
      "idiot: 5.2509\n",
      "shit: 5.2149\n",
      "ass: 5.1603\n",
      "peggy: 4.8790\n",
      "fucking: 4.6901\n",
      "bitch: 4.6859\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el mejor modelo con threshold\n",
    "threshold = 0.59\n",
    "\n",
    "# Predicciones en entrenamiento\n",
    "y_prob_train = best_model.predict_proba(X_train_reduced)[:, 1]\n",
    "y_pred_train_thresholded = (y_prob_train >= threshold).astype(int)\n",
    "\n",
    "# Predicciones en prueba\n",
    "y_prob_test = best_model.predict_proba(X_test_reduced)[:, 1]\n",
    "y_pred_test_thresholded = (y_prob_test >= threshold).astype(int)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train_thresholded)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test_thresholded)\n",
    "overfitting_percentage = ((accuracy_train - accuracy_test) / accuracy_train) * 100\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nResultados finales:\")\n",
    "print(f\"Accuracy en entrenamiento (threshold={threshold}): {accuracy_train:.4f}\")\n",
    "print(f\"Accuracy en prueba (threshold={threshold}): {accuracy_test:.4f}\")\n",
    "print(f\"Porcentaje de overfitting: {overfitting_percentage:.2f}%\")\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_test, y_pred_test_thresholded))\n",
    "\n",
    "# Visualizar las características más importantes\n",
    "if hasattr(selector, 'get_feature_names_out'):\n",
    "    feature_names = vectorizer_2.get_feature_names_out()\n",
    "    selected_features = selector.get_support()\n",
    "    important_features = [(feature_names[i], selector.scores_[i]) \n",
    "                         for i in range(len(feature_names)) \n",
    "                         if selected_features[i]]\n",
    "    \n",
    "    important_features.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nTop 10 características más importantes:\")\n",
    "    for feature, score in important_features[:10]:\n",
    "        print(f\"{feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ensemble_model.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar el vectorizador\n",
    "joblib.dump(vectorizer_2, 'vectorizer_2.pkl')\n",
    "\n",
    "# Guardar el selector de características\n",
    "joblib.dump(selector, 'feature_selector.pkl')\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "joblib.dump(best_model, 'ensemble_model.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
