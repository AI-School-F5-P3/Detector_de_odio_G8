{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Celda 1] - Importación de librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Any_Hate</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsNationalist</th>\n",
       "      <th>IsSexist</th>\n",
       "      <th>IsHomophobic</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "      <th>IsRadicalism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people would take step back make case anyone e...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>law enforcement trained shoot apprehend traine...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dont reckon life matter banner held white cunt...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>large number people like police officer called...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arab dude absolutely right shot 6 extra time s...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Cleaned_Text  Any_Hate  IsToxic  \\\n",
       "0  people would take step back make case anyone e...     False    False   \n",
       "1  law enforcement trained shoot apprehend traine...      True     True   \n",
       "2  dont reckon life matter banner held white cunt...      True     True   \n",
       "3  large number people like police officer called...     False    False   \n",
       "4  arab dude absolutely right shot 6 extra time s...     False    False   \n",
       "\n",
       "   IsAbusive  IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  \\\n",
       "0      False     False          False      False         False     False   \n",
       "1       True     False          False      False         False     False   \n",
       "2       True     False          False       True         False     False   \n",
       "3      False     False          False      False         False     False   \n",
       "4      False     False          False      False         False     False   \n",
       "\n",
       "   IsNationalist  IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
       "0          False     False         False            False         False  \n",
       "1          False     False         False            False         False  \n",
       "2          False     False         False            False         False  \n",
       "3          False     False         False            False         False  \n",
       "4          False     False         False            False         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Información del dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Cleaned_Text     1000 non-null   object\n",
      " 1   Any_Hate         1000 non-null   bool  \n",
      " 2   IsToxic          1000 non-null   bool  \n",
      " 3   IsAbusive        1000 non-null   bool  \n",
      " 4   IsThreat         1000 non-null   bool  \n",
      " 5   IsProvocative    1000 non-null   bool  \n",
      " 6   IsObscene        1000 non-null   bool  \n",
      " 7   IsHatespeech     1000 non-null   bool  \n",
      " 8   IsRacist         1000 non-null   bool  \n",
      " 9   IsNationalist    1000 non-null   bool  \n",
      " 10  IsSexist         1000 non-null   bool  \n",
      " 11  IsHomophobic     1000 non-null   bool  \n",
      " 12  IsReligiousHate  1000 non-null   bool  \n",
      " 13  IsRadicalism     1000 non-null   bool  \n",
      "dtypes: bool(13), object(1)\n",
      "memory usage: 20.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Celda 2] - Carga de datos\n",
    "# Leemos el dataset\n",
    "df = pd.read_csv('../data/cleaned_data.csv')\n",
    "\n",
    "# Mostramos las primeras filas y la información del dataset\n",
    "print(\"Primeras filas del dataset:\")\n",
    "display(df.head())\n",
    "print(\"\\nInformación del dataset:\")\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores nulos por columna antes de la limpieza:\n",
      "Cleaned_Text       0\n",
      "Any_Hate           0\n",
      "IsToxic            0\n",
      "IsAbusive          0\n",
      "IsThreat           0\n",
      "IsProvocative      0\n",
      "IsObscene          0\n",
      "IsHatespeech       0\n",
      "IsRacist           0\n",
      "IsNationalist      0\n",
      "IsSexist           0\n",
      "IsHomophobic       0\n",
      "IsReligiousHate    0\n",
      "IsRadicalism       0\n",
      "dtype: int64\n",
      "\n",
      "Total de filas antes de la limpieza: 1000\n",
      "\n",
      "Estadísticas finales:\n",
      "--------------------------------------------------\n",
      "Distribución de mensajes dañinos vs no dañinos:\n",
      "is_harmful\n",
      "0    0.538\n",
      "1    0.462\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Distribución por tipo de contenido dañino:\n",
      "IsToxic: 462 casos (46.20%)\n",
      "IsAbusive: 353 casos (35.30%)\n",
      "IsThreat: 21 casos (2.10%)\n",
      "IsProvocative: 161 casos (16.10%)\n",
      "IsObscene: 100 casos (10.00%)\n",
      "IsHatespeech: 138 casos (13.80%)\n",
      "IsRacist: 125 casos (12.50%)\n",
      "IsNationalist: 8 casos (0.80%)\n",
      "IsSexist: 1 casos (0.10%)\n",
      "IsHomophobic: 0 casos (0.00%)\n",
      "IsReligiousHate: 12 casos (1.20%)\n",
      "IsRadicalism: 0 casos (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# [Celda 3] - Preparación y limpieza de datos\n",
    "\n",
    "# Primero mostramos información sobre valores nulos en el dataset\n",
    "print(\"Valores nulos por columna antes de la limpieza:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nTotal de filas antes de la limpieza:\", len(df))\n",
    "\n",
    "# Definimos las columnas que indican contenido dañino\n",
    "toxic_columns = ['IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', \n",
    "                'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist',\n",
    "                'IsSexist', 'IsHomophobic', 'IsReligiousHate', 'IsRadicalism']\n",
    "\n",
    "# Convertir los valores \"TRUE\"/\"FALSE\" a 1/0 en las columnas tóxicas (si es necesario)\n",
    "df[toxic_columns] = df[toxic_columns].replace({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "# Creamos una nueva columna que será 1 si cualquiera de las categorías es 1\n",
    "df['is_harmful'] = df[toxic_columns].any(axis=1).astype(int)\n",
    "\n",
    "# Preparamos X (textos) e y (etiquetas)\n",
    "X = df['Cleaned_Text']\n",
    "y = df['is_harmful']\n",
    "\n",
    "# Mostramos estadísticas finales\n",
    "print(\"\\nEstadísticas finales:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Distribución de mensajes dañinos vs no dañinos:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "print(\"\\nDistribución por tipo de contenido dañino:\")\n",
    "for col in toxic_columns:\n",
    "    positivos = df[col].sum()\n",
    "    porcentaje = (positivos / len(df)) * 100\n",
    "    print(f\"{col}: {positivos} casos ({porcentaje:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 850\n",
      "Tamaño del conjunto de prueba: 150\n"
     ]
    }
   ],
   "source": [
    "# [Celda 4] - División de datos\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y  # Mantenemos la proporción de clases\n",
    ")\n",
    "\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(x_train))\n",
    "print(\"Tamaño del conjunto de prueba:\", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la matriz de características de entrenamiento: (850, 1421)\n"
     ]
    }
   ],
   "source": [
    "# [Celda 5] - Vectorización del texto\n",
    "# Convertimos el texto a una matriz de características TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limitamos el número de características\n",
    "    min_df=2,          # Ignoramos términos que aparecen en menos de 2 documentos\n",
    "    stop_words='english'  # Removemos stopwords en inglés\n",
    ")\n",
    "\n",
    "# Transformamos los textos\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "print(\"Dimensiones de la matriz de características de entrenamiento:\", x_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo final entrenado con los mejores hiperparámetros!\n"
     ]
    }
   ],
   "source": [
    "# Extraer los mejores parámetros\n",
    "\n",
    "# Crear el modelo SVM con los mejores hiperparámetros hardcodeados\n",
    "final_svm_model = SVC(\n",
    "    C=109.42844096035671,\n",
    "    kernel='rbf',\n",
    "    gamma=1.1662316858478285,\n",
    "    probability=True,\n",
    "    random_state=42,\n",
    "    class_weight=None,\n",
    "    tol=0.00043366566542372227,\n",
    "    shrinking=False,\n",
    "    max_iter=1697\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento completo\n",
    "final_svm_model.fit(x_train_vectorized, y_train)\n",
    "\n",
    "print(\"Modelo final entrenado con los mejores hiperparámetros!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en conjunto de prueba (con umbral 0.46): 0.7266666666666667\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.75        81\n",
      "           1       0.72      0.67      0.69        69\n",
      "\n",
      "    accuracy                           0.73       150\n",
      "   macro avg       0.73      0.72      0.72       150\n",
      "weighted avg       0.73      0.73      0.73       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicciones de probabilidades para las clases\n",
    "y_prob = final_svm_model.predict_proba(x_test_vectorized)[:, 1]  # Probabilidad de la clase positiva (por ejemplo, clase 1)\n",
    "\n",
    "# Ajuste del umbral de decisión\n",
    "threshold = 0.46  # Cambia este valor según lo que desees probar\n",
    "y_pred = (y_prob >= threshold).astype(int)  # Predicciones basadas en el umbral\n",
    "\n",
    "# Métricas de rendimiento\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy en conjunto de prueba (con umbral {threshold}):\", accuracy)\n",
    "print(\"Reporte de clasificación:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-11 17:43:54,794] A new study created in memory with name: no-name-6e9c64d9-8267-49ee-8077-255bd7b3e344\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,810] Trial 0 finished with value: 0.6753139567676968 and parameters: {'alpha': 19.359799819449584, 'fit_prior': False}. Best is trial 0 with value: 0.6753139567676968.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,825] Trial 1 finished with value: 0.6658911063554472 and parameters: {'alpha': 0.018588854323729007, 'fit_prior': False}. Best is trial 0 with value: 0.6753139567676968.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,836] Trial 2 finished with value: 0.6023490767929128 and parameters: {'alpha': 14.246983555791033, 'fit_prior': True}. Best is trial 0 with value: 0.6753139567676968.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,851] Trial 3 finished with value: 0.6882330496524495 and parameters: {'alpha': 4.602962179814199, 'fit_prior': False}. Best is trial 3 with value: 0.6882330496524495.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,864] Trial 4 finished with value: 0.6929403274772309 and parameters: {'alpha': 2.0229700028546844, 'fit_prior': True}. Best is trial 4 with value: 0.6929403274772309.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,873] Trial 5 finished with value: 0.6506038587236019 and parameters: {'alpha': 0.0006969488722283292, 'fit_prior': False}. Best is trial 4 with value: 0.6929403274772309.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,882] Trial 6 finished with value: 0.6494260024220707 and parameters: {'alpha': 0.001164628035029057, 'fit_prior': False}. Best is trial 4 with value: 0.6929403274772309.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,890] Trial 7 finished with value: 0.6506038587236019 and parameters: {'alpha': 7.339970561351191e-05, 'fit_prior': False}. Best is trial 4 with value: 0.6929403274772309.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,901] Trial 8 finished with value: 0.6811700592246055 and parameters: {'alpha': 0.236302487713164, 'fit_prior': True}. Best is trial 4 with value: 0.6929403274772309.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,909] Trial 9 finished with value: 0.6776655219894159 and parameters: {'alpha': 0.46744725395575937, 'fit_prior': False}. Best is trial 4 with value: 0.6929403274772309.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,924] Trial 10 finished with value: 0.5376457804542213 and parameters: {'alpha': 82.67817944140309, 'fit_prior': True}. Best is trial 4 with value: 0.6929403274772309.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,937] Trial 11 finished with value: 0.6988420511289172 and parameters: {'alpha': 0.7915443361510179, 'fit_prior': True}. Best is trial 11 with value: 0.6988420511289172.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,949] Trial 12 finished with value: 0.6799963503044179 and parameters: {'alpha': 0.2605711522251888, 'fit_prior': True}. Best is trial 11 with value: 0.6988420511289172.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,963] Trial 13 finished with value: 0.665878664211417 and parameters: {'alpha': 0.013117389271764049, 'fit_prior': True}. Best is trial 11 with value: 0.6988420511289172.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,975] Trial 14 finished with value: 0.6929527696212611 and parameters: {'alpha': 1.4428776587168588, 'fit_prior': True}. Best is trial 11 with value: 0.6988420511289172.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:54,988] Trial 15 finished with value: 0.7059050415567611 and parameters: {'alpha': 1.0650934668080638, 'fit_prior': True}. Best is trial 15 with value: 0.7059050415567611.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,000] Trial 16 finished with value: 0.6788184940028866 and parameters: {'alpha': 0.0872242875617257, 'fit_prior': True}. Best is trial 15 with value: 0.7059050415567611.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,014] Trial 17 finished with value: 0.6553028417856965 and parameters: {'alpha': 0.0030636340852760314, 'fit_prior': True}. Best is trial 15 with value: 0.7059050415567611.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,030] Trial 18 finished with value: 0.6470702898190083 and parameters: {'alpha': 1.4548046828990586e-05, 'fit_prior': True}. Best is trial 15 with value: 0.7059050415567611.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,042] Trial 19 finished with value: 0.6729416546392608 and parameters: {'alpha': 0.06306591284653999, 'fit_prior': True}. Best is trial 15 with value: 0.7059050415567611.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,056] Trial 20 finished with value: 0.7011936163506363 and parameters: {'alpha': 0.8248464209659434, 'fit_prior': True}. Best is trial 15 with value: 0.7059050415567611.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,071] Trial 21 finished with value: 0.7070787504769488 and parameters: {'alpha': 1.0359637740543461, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,084] Trial 22 finished with value: 0.6729416546392608 and parameters: {'alpha': 6.695978940433635, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,096] Trial 23 finished with value: 0.5376457804542213 and parameters: {'alpha': 74.6025384886953, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,110] Trial 24 finished with value: 0.6964821911445114 and parameters: {'alpha': 3.236939083861764, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,122] Trial 25 finished with value: 0.6811742066059491 and parameters: {'alpha': 0.1545174916455473, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,135] Trial 26 finished with value: 0.7011977637319796 and parameters: {'alpha': 0.7185417483856135, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,150] Trial 27 finished with value: 0.6729375072579172 and parameters: {'alpha': 0.03567321163979323, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,164] Trial 28 finished with value: 0.5823462565735995 and parameters: {'alpha': 17.9024100182575, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,178] Trial 29 finished with value: 0.6658828115927603 and parameters: {'alpha': 7.5548805799111545, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,192] Trial 30 finished with value: 0.6611713863866355 and parameters: {'alpha': 0.007423951243895454, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,207] Trial 31 finished with value: 0.7011936163506363 and parameters: {'alpha': 0.8287384083854331, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,221] Trial 32 finished with value: 0.696478043763168 and parameters: {'alpha': 0.5372642755957615, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,233] Trial 33 finished with value: 0.5576361585295045 and parameters: {'alpha': 28.57627812352329, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,245] Trial 34 finished with value: 0.6870593407322616 and parameters: {'alpha': 2.487451817181978, 'fit_prior': False}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,257] Trial 35 finished with value: 0.6776406377013554 and parameters: {'alpha': 0.10100510395277743, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,272] Trial 36 finished with value: 0.6941306259227923 and parameters: {'alpha': 1.4866134981032646, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,285] Trial 37 finished with value: 0.6694205278786973 and parameters: {'alpha': 0.03163519235094827, 'fit_prior': False}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,299] Trial 38 finished with value: 0.634113870502165 and parameters: {'alpha': 10.448652632875486, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,311] Trial 39 finished with value: 0.6905887622555119 and parameters: {'alpha': 3.2875887699402995, 'fit_prior': False}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,323] Trial 40 finished with value: 0.6858731896680436 and parameters: {'alpha': 0.3200670698637422, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,336] Trial 41 finished with value: 0.7035493289536987 and parameters: {'alpha': 0.9564335826686292, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,352] Trial 42 finished with value: 0.7035493289536987 and parameters: {'alpha': 0.8812957962660556, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,364] Trial 43 finished with value: 0.5494160487068465 and parameters: {'alpha': 33.75726113128435, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,378] Trial 44 finished with value: 0.6964863385258547 and parameters: {'alpha': 1.6558939836143889, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,390] Trial 45 finished with value: 0.6811742066059491 and parameters: {'alpha': 0.15478836013011912, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,403] Trial 46 finished with value: 0.676491813069228 and parameters: {'alpha': 0.42808019371134176, 'fit_prior': False}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,415] Trial 47 finished with value: 0.6882413444151364 and parameters: {'alpha': 5.279522671682321, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,429] Trial 48 finished with value: 0.7070787504769488 and parameters: {'alpha': 1.0395608855075062, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n",
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_440\\1044462155.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
      "[I 2024-11-11 17:43:55,443] Trial 49 finished with value: 0.6799963503044179 and parameters: {'alpha': 0.25212432381152944, 'fit_prior': True}. Best is trial 21 with value: 0.7070787504769488.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros para MultinomialNB:\n",
      "{'alpha': 1.0359637740543461, 'fit_prior': True}\n",
      "Mejor precisión: 0.7070787504769488\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Definir la función objetivo para Optuna con MultinomialNB\n",
    "def objective_nb(trial):\n",
    "    # Definir los hiperparámetros a optimizar\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e2)  # Parámetro de suavizado\n",
    "    fit_prior = trial.suggest_categorical('fit_prior', [True, False])  # Ajustar o no las probabilidades previas\n",
    "    \n",
    "    # Crear el modelo Naive Bayes con los hiperparámetros seleccionados\n",
    "    nb_model = MultinomialNB(alpha=alpha, fit_prior=fit_prior)\n",
    "    \n",
    "    # Evaluar el modelo con validación cruzada\n",
    "    scores = cross_val_score(nb_model, x_train_vectorized, y_train, cv=3, scoring='accuracy')\n",
    "    accuracy = scores.mean()  # Promedio de accuracy\n",
    "    \n",
    "    return accuracy  # Optuna maximiza este valor\n",
    "\n",
    "# Crear un estudio de optimización para MultinomialNB\n",
    "study_nb = optuna.create_study(direction='maximize')\n",
    "study_nb.optimize(objective_nb, n_trials=50)  # Ajusta el número de pruebas según el tiempo disponible\n",
    "\n",
    "# Resultados de la mejor prueba para MultinomialNB\n",
    "print(\"Mejores hiperparámetros para MultinomialNB:\")\n",
    "print(study_nb.best_trial.params)\n",
    "print(\"Mejor precisión:\", study_nb.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en conjunto de prueba para MultinomialNB: 0.6933333333333334\n",
      "Reporte de clasificación para MultinomialNB:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73        81\n",
      "           1       0.69      0.59      0.64        69\n",
      "\n",
      "    accuracy                           0.69       150\n",
      "   macro avg       0.69      0.69      0.69       150\n",
      "weighted avg       0.69      0.69      0.69       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraer los mejores hiperparámetros del estudio de MultinomialNB\n",
    "best_params_nb = study_nb.best_trial.params\n",
    "\n",
    "# Crear el modelo final con los mejores hiperparámetros\n",
    "final_nb_model = MultinomialNB(\n",
    "    alpha=best_params_nb['alpha'],\n",
    "    fit_prior=best_params_nb['fit_prior']\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento completo\n",
    "final_nb_model.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_nb = final_nb_model.predict(x_test_vectorized)\n",
    "\n",
    "# Calcular y mostrar la precisión y el reporte de clasificación\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "report_nb = classification_report(y_test, y_pred_nb)\n",
    "\n",
    "print(\"Accuracy en conjunto de prueba para MultinomialNB:\", accuracy_nb)\n",
    "print(\"Reporte de clasificación para MultinomialNB:\\n\", report_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-11 17:43:55,471] A new study created in memory with name: no-name-9ceae4ae-6ac6-4cb6-ab2e-2a867f3f4094\n",
      "[I 2024-11-11 17:43:56,011] Trial 0 finished with value: 0.5376457804542213 and parameters: {'n_estimators': 191, 'max_depth': 36, 'min_samples_split': 8, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 0 with value: 0.5376457804542213.\n",
      "[I 2024-11-11 17:43:56,863] Trial 1 finished with value: 0.6753222515303837 and parameters: {'n_estimators': 217, 'max_depth': 39, 'min_samples_split': 14, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.6753222515303837.\n",
      "[I 2024-11-11 17:43:57,162] Trial 2 finished with value: 0.588231390699912 and parameters: {'n_estimators': 99, 'max_depth': 44, 'min_samples_split': 5, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.6753222515303837.\n",
      "[I 2024-11-11 17:43:57,902] Trial 3 finished with value: 0.5517593191658786 and parameters: {'n_estimators': 242, 'max_depth': 6, 'min_samples_split': 16, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.6753222515303837.\n",
      "[I 2024-11-11 17:43:58,453] Trial 4 finished with value: 0.5376457804542213 and parameters: {'n_estimators': 214, 'max_depth': 48, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 1 with value: 0.6753222515303837.\n",
      "[I 2024-11-11 17:43:58,990] Trial 5 finished with value: 0.6859105161001344 and parameters: {'n_estimators': 108, 'max_depth': 47, 'min_samples_split': 15, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 5 with value: 0.6859105161001344.\n",
      "[I 2024-11-11 17:43:59,790] Trial 6 finished with value: 0.6918039449891339 and parameters: {'n_estimators': 164, 'max_depth': 34, 'min_samples_split': 14, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.6918039449891339.\n",
      "[I 2024-11-11 17:43:59,992] Trial 7 finished with value: 0.5517551717845351 and parameters: {'n_estimators': 70, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 6 with value: 0.6918039449891339.\n",
      "[I 2024-11-11 17:44:00,863] Trial 8 finished with value: 0.6753222515303837 and parameters: {'n_estimators': 226, 'max_depth': 39, 'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 6 with value: 0.6918039449891339.\n",
      "[I 2024-11-11 17:44:01,848] Trial 9 finished with value: 0.6729623915459779 and parameters: {'n_estimators': 283, 'max_depth': 26, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.6918039449891339.\n",
      "[I 2024-11-11 17:44:02,357] Trial 10 finished with value: 0.6459007282801639 and parameters: {'n_estimators': 147, 'max_depth': 23, 'min_samples_split': 19, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.6918039449891339.\n",
      "[I 2024-11-11 17:44:02,955] Trial 11 finished with value: 0.6870883724016656 and parameters: {'n_estimators': 129, 'max_depth': 32, 'min_samples_split': 13, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 6 with value: 0.6918039449891339.\n",
      "[I 2024-11-11 17:44:03,680] Trial 12 finished with value: 0.6929693591466348 and parameters: {'n_estimators': 157, 'max_depth': 32, 'min_samples_split': 12, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.6929693591466348.\n",
      "[I 2024-11-11 17:44:04,377] Trial 13 finished with value: 0.6858939265747607 and parameters: {'n_estimators': 167, 'max_depth': 19, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.6929693591466348.\n",
      "[I 2024-11-11 17:44:05,192] Trial 14 finished with value: 0.695312629605667 and parameters: {'n_estimators': 172, 'max_depth': 30, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 14 with value: 0.695312629605667.\n",
      "[I 2024-11-11 17:44:06,355] Trial 15 finished with value: 0.6882496391778231 and parameters: {'n_estimators': 261, 'max_depth': 21, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 14 with value: 0.695312629605667.\n",
      "[I 2024-11-11 17:44:07,228] Trial 16 finished with value: 0.692961064383948 and parameters: {'n_estimators': 185, 'max_depth': 30, 'min_samples_split': 11, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 14 with value: 0.695312629605667.\n",
      "[I 2024-11-11 17:44:07,440] Trial 17 finished with value: 0.6717803878631031 and parameters: {'n_estimators': 51, 'max_depth': 16, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 14 with value: 0.695312629605667.\n",
      "[I 2024-11-11 17:44:07,969] Trial 18 finished with value: 0.6765001078319149 and parameters: {'n_estimators': 125, 'max_depth': 26, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 14 with value: 0.695312629605667.\n",
      "[I 2024-11-11 17:44:08,959] Trial 19 finished with value: 0.695312629605667 and parameters: {'n_estimators': 194, 'max_depth': 41, 'min_samples_split': 12, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 14 with value: 0.695312629605667.\n",
      "[I 2024-11-11 17:44:09,967] Trial 20 finished with value: 0.6964863385258547 and parameters: {'n_estimators': 196, 'max_depth': 39, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 20 with value: 0.6964863385258547.\n",
      "[I 2024-11-11 17:44:10,988] Trial 21 finished with value: 0.6964904859071982 and parameters: {'n_estimators': 196, 'max_depth': 42, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:12,008] Trial 22 finished with value: 0.6941347733041358 and parameters: {'n_estimators': 198, 'max_depth': 42, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:12,993] Trial 23 finished with value: 0.6847326597986032 and parameters: {'n_estimators': 238, 'max_depth': 50, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:13,717] Trial 24 finished with value: 0.6906012043995422 and parameters: {'n_estimators': 142, 'max_depth': 36, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:15,005] Trial 25 finished with value: 0.6882496391778231 and parameters: {'n_estimators': 262, 'max_depth': 29, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:15,610] Trial 26 finished with value: 0.6753222515303837 and parameters: {'n_estimators': 175, 'max_depth': 45, 'min_samples_split': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:16,625] Trial 27 finished with value: 0.695312629605667 and parameters: {'n_estimators': 202, 'max_depth': 37, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:17,381] Trial 28 finished with value: 0.6847202176545729 and parameters: {'n_estimators': 180, 'max_depth': 43, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:17,932] Trial 29 finished with value: 0.5376457804542213 and parameters: {'n_estimators': 206, 'max_depth': 34, 'min_samples_split': 9, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:19,222] Trial 30 finished with value: 0.6859022213374475 and parameters: {'n_estimators': 233, 'max_depth': 39, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:20,198] Trial 31 finished with value: 0.695312629605667 and parameters: {'n_estimators': 186, 'max_depth': 41, 'min_samples_split': 12, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:21,204] Trial 32 finished with value: 0.6929652117652915 and parameters: {'n_estimators': 194, 'max_depth': 40, 'min_samples_split': 12, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:22,404] Trial 33 finished with value: 0.6847160702732294 and parameters: {'n_estimators': 217, 'max_depth': 45, 'min_samples_split': 13, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:22,870] Trial 34 finished with value: 0.6247034622339455 and parameters: {'n_estimators': 154, 'max_depth': 37, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:24,238] Trial 35 finished with value: 0.6894357902420412 and parameters: {'n_estimators': 256, 'max_depth': 47, 'min_samples_split': 16, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:25,057] Trial 36 finished with value: 0.6764835183065413 and parameters: {'n_estimators': 218, 'max_depth': 33, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:25,589] Trial 37 finished with value: 0.6270550274556644 and parameters: {'n_estimators': 171, 'max_depth': 50, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:26,152] Trial 38 finished with value: 0.6823852419582276 and parameters: {'n_estimators': 133, 'max_depth': 44, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:26,572] Trial 39 finished with value: 0.6765084025946018 and parameters: {'n_estimators': 105, 'max_depth': 38, 'min_samples_split': 14, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:27,292] Trial 40 finished with value: 0.5787960981436321 and parameters: {'n_estimators': 246, 'max_depth': 41, 'min_samples_split': 16, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:28,311] Trial 41 finished with value: 0.690609499162229 and parameters: {'n_estimators': 203, 'max_depth': 36, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:29,372] Trial 42 finished with value: 0.6941347733041358 and parameters: {'n_estimators': 209, 'max_depth': 36, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:30,346] Trial 43 finished with value: 0.6882537865591666 and parameters: {'n_estimators': 191, 'max_depth': 30, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:31,364] Trial 44 finished with value: 0.6894482323860714 and parameters: {'n_estimators': 228, 'max_depth': 27, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:32,103] Trial 45 finished with value: 0.6894482323860714 and parameters: {'n_estimators': 158, 'max_depth': 35, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:32,635] Trial 46 finished with value: 0.5964763848106305 and parameters: {'n_estimators': 175, 'max_depth': 47, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:33,738] Trial 47 finished with value: 0.692961064383948 and parameters: {'n_estimators': 219, 'max_depth': 39, 'min_samples_split': 13, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:34,485] Trial 48 finished with value: 0.6717845352444467 and parameters: {'n_estimators': 197, 'max_depth': 32, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 21 with value: 0.6964904859071982.\n",
      "[I 2024-11-11 17:44:35,038] Trial 49 finished with value: 0.6682675558652268 and parameters: {'n_estimators': 167, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 21 with value: 0.6964904859071982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros para RandomForestClassifier:\n",
      "{'n_estimators': 196, 'max_depth': 42, 'min_samples_split': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}\n",
      "Mejor precisión: 0.6964904859071982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definir la función objetivo para Optuna con RandomForestClassifier\n",
    "def objective_rf(trial):\n",
    "    # Definir los hiperparámetros a optimizar\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300)  # Número de árboles\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 50)  # Profundidad máxima de cada árbol\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)  # Tamaño mínimo para hacer split\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)  # Tamaño mínimo de muestras en una hoja\n",
    "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2'])  # Características a considerar\n",
    "    \n",
    "    # Crear el modelo Random Forest con los hiperparámetros seleccionados\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Evaluar el modelo con validación cruzada\n",
    "    scores = cross_val_score(rf_model, x_train_vectorized, y_train, cv=3, scoring='accuracy')\n",
    "    accuracy = scores.mean()  # Promedio de accuracy\n",
    "    \n",
    "    return accuracy  # Optuna maximiza este valor\n",
    "\n",
    "# Crear un estudio de optimización para RandomForestClassifier\n",
    "study_rf = optuna.create_study(direction='maximize')\n",
    "study_rf.optimize(objective_rf, n_trials=50)  # Ajusta el número de pruebas según el tiempo disponible\n",
    "\n",
    "# Resultados de la mejor prueba para RandomForestClassifier\n",
    "print(\"Mejores hiperparámetros para RandomForestClassifier:\")\n",
    "print(study_rf.best_trial.params)\n",
    "print(\"Mejor precisión:\", study_rf.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en conjunto de prueba para RandomForestClassifier: 0.72\n",
      "Reporte de clasificación para RandomForestClassifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.90      0.78        81\n",
      "           1       0.81      0.51      0.62        69\n",
      "\n",
      "    accuracy                           0.72       150\n",
      "   macro avg       0.75      0.70      0.70       150\n",
      "weighted avg       0.74      0.72      0.71       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear el modelo final con los mejores hiperparámetros\n",
    "final_rf_model = RandomForestClassifier(\n",
    "    n_estimators=235,\n",
    "    max_depth=44,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con el conjunto de entrenamiento completo\n",
    "final_rf_model.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_rf = final_rf_model.predict(x_test_vectorized)\n",
    "\n",
    "# Calcular y mostrar la precisión y el reporte de clasificación\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "report_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Accuracy en conjunto de prueba para RandomForestClassifier:\", accuracy_rf)\n",
    "print(\"Reporte de clasificación para RandomForestClassifier:\\n\", report_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en conjunto de prueba para el ensemble: 0.7333333333333333\n",
      "Reporte de clasificación para el ensemble:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.78        81\n",
      "           1       0.77      0.59      0.67        69\n",
      "\n",
      "    accuracy                           0.73       150\n",
      "   macro avg       0.74      0.72      0.72       150\n",
      "weighted avg       0.74      0.73      0.73       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear el ensemble con los modelos finales\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('random_forest', final_rf_model),\n",
    "        ('naive_bayes', final_nb_model),\n",
    "        ('svm', final_svm_model)\n",
    "    ],\n",
    "    voting='soft'  # Votación suave para promediar probabilidades\n",
    ")\n",
    "\n",
    "# Entrenar el ensemble con el conjunto de entrenamiento\n",
    "ensemble_model.fit(x_train_vectorized, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_ensemble = ensemble_model.predict(x_test_vectorized)\n",
    "\n",
    "# Calcular y mostrar la precisión y el reporte de clasificación\n",
    "accuracy_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
    "report_ensemble = classification_report(y_test, y_pred_ensemble)\n",
    "\n",
    "print(\"Accuracy en conjunto de prueba para el ensemble:\", accuracy_ensemble)\n",
    "print(\"Reporte de clasificación para el ensemble:\\n\", report_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en conjunto de entrenamiento para el ensemble con threshold de 0.759 : 0.7658823529411765\n",
      "Accuracy en conjunto de prueba para el ensemble con threshold de 0.759 : 0.5866666666666667\n",
      "Porcentaje de overfitting del ensemble: 23.399897593445978 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de entrenamiento utilizando el nuevo threshold\n",
    "y_prob_train_ensemble = ensemble_model.predict_proba(x_train_vectorized)[:, 1]  # Probabilidades de la clase positiva\n",
    "\n",
    "# Definir un nuevo threshold\n",
    "threshold = 0.759 # Puedes cambiar este valor según lo que necesites\n",
    "\n",
    "# Aplicar el threshold para obtener las predicciones\n",
    "y_pred_train_ensemble_thresholded = (y_prob_train_ensemble >= threshold).astype(int)\n",
    "\n",
    "# Calcular la precisión en el conjunto de entrenamiento con el nuevo threshold\n",
    "accuracy_train_ensemble = accuracy_score(y_train, y_pred_train_ensemble_thresholded)\n",
    "\n",
    "# Evaluar en el conjunto de prueba con el nuevo threshold\n",
    "y_prob_test_ensemble = ensemble_model.predict_proba(x_test_vectorized)[:, 1]\n",
    "y_pred_test_ensemble_thresholded = (y_prob_test_ensemble >= threshold).astype(int)\n",
    "\n",
    "# Calcular la precisión en el conjunto de prueba con el nuevo threshold\n",
    "accuracy_test_ensemble = accuracy_score(y_test, y_pred_test_ensemble_thresholded)\n",
    "\n",
    "# Calcular el porcentaje de overfitting\n",
    "overfitting_percentage = ((accuracy_train_ensemble - accuracy_test_ensemble) / accuracy_train_ensemble) * 100\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Accuracy en conjunto de entrenamiento para el ensemble con threshold de\", threshold, \":\", accuracy_train_ensemble)\n",
    "print(\"Accuracy en conjunto de prueba para el ensemble con threshold de\", threshold, \":\", accuracy_test_ensemble)\n",
    "print(\"Porcentaje de overfitting del ensemble:\", overfitting_percentage, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones después de la reducción:\n",
      "Entrenamiento: (891, 1000)\n",
      "Validación: (99, 1000)\n",
      "Prueba: (10, 1000)\n",
      "Época 1, Accuracy en validación: 0.7677\n",
      "Época 2, Accuracy en validación: 0.7677\n",
      "Época 3, Accuracy en validación: 0.7677\n",
      "Época 4, Accuracy en validación: 0.7677\n",
      "Early stopping activado en época 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# 1. Primero dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "x_temp, x_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.01,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Separamos el conjunto temporal en entrenamiento y validación\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_temp, y_temp,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# 2. Vectorizamos los datos\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "# Vectorizamos cada conjunto por separado\n",
    "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "x_val_vectorized = vectorizer.transform(x_val)\n",
    "x_test_vectorized = vectorizer.transform(x_test)\n",
    "\n",
    "# 3. Aplicamos la reducción de dimensionalidad\n",
    "selector = SelectKBest(score_func=chi2, k=1000)\n",
    "x_train_reduced = selector.fit_transform(x_train_vectorized, y_train)\n",
    "x_val_reduced = selector.transform(x_val_vectorized)\n",
    "x_test_reduced = selector.transform(x_test_vectorized)\n",
    "\n",
    "print(\"Dimensiones después de la reducción:\")\n",
    "print(\"Entrenamiento:\", x_train_reduced.shape)\n",
    "print(\"Validación:\", x_val_reduced.shape)\n",
    "print(\"Prueba:\", x_test_reduced.shape)\n",
    "\n",
    "# 4. Creamos y entrenamos el ensemble con bagging\n",
    "bagged_ensemble = BaggingClassifier(\n",
    "    estimator=VotingClassifier(\n",
    "        estimators=[\n",
    "            ('random_forest', final_rf_model),\n",
    "            ('naive_bayes', final_nb_model),\n",
    "            ('svm', final_svm_model)\n",
    "        ],\n",
    "        voting='soft'\n",
    "    ),\n",
    "    n_estimators=10,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5. Implementamos early stopping\n",
    "class EarlyStoppingMonitor:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.best_model = None\n",
    "        \n",
    "    def check(self, model, val_score):\n",
    "        if self.best_score is None or val_score > self.best_score + self.min_delta:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "            self.best_model = model\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "# Entrenamiento con early stopping\n",
    "monitor = EarlyStoppingMonitor(patience=3)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    # Entrenar el modelo\n",
    "    bagged_ensemble.fit(x_train_reduced, y_train)\n",
    "    \n",
    "    # Evaluar en el conjunto de validación\n",
    "    val_accuracy = bagged_ensemble.score(x_val_reduced, y_val)\n",
    "    print(f\"Época {epoch + 1}, Accuracy en validación: {val_accuracy:.4f}\")\n",
    "    \n",
    "    if monitor.check(bagged_ensemble, val_accuracy):\n",
    "        print(\"Early stopping activado en época\", epoch + 1)\n",
    "        break\n",
    "\n",
    "# Usar el mejor modelo encontrado\n",
    "best_model = monitor.best_model if monitor.best_model is not None else bagged_ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados finales:\n",
      "Accuracy en entrenamiento (threshold=0.59): 0.8204\n",
      "Accuracy en prueba (threshold=0.59): 0.6000\n",
      "Porcentaje de overfitting: 26.87%\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.80      0.67         5\n",
      "           1       0.67      0.40      0.50         5\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.62      0.60      0.58        10\n",
      "weighted avg       0.62      0.60      0.58        10\n",
      "\n",
      "\n",
      "Top 10 características más importantes:\n",
      "run: 11.2102\n",
      "fuck: 9.5244\n",
      "idiot: 8.7847\n",
      "shit: 6.7682\n",
      "as: 5.8536\n",
      "bitch: 5.6284\n",
      "fucking: 5.5250\n",
      "dumb: 5.3711\n",
      "peggy: 5.1694\n",
      "thug: 4.6905\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el mejor modelo con threshold\n",
    "threshold = 0.59\n",
    "\n",
    "# Predicciones en entrenamiento\n",
    "y_prob_train = best_model.predict_proba(x_train_reduced)[:, 1]\n",
    "y_pred_train_thresholded = (y_prob_train >= threshold).astype(int)\n",
    "\n",
    "# Predicciones en prueba\n",
    "y_prob_test = best_model.predict_proba(x_test_reduced)[:, 1]\n",
    "y_pred_test_thresholded = (y_prob_test >= threshold).astype(int)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train_thresholded)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test_thresholded)\n",
    "overfitting_percentage = ((accuracy_train - accuracy_test) / accuracy_train) * 100\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nResultados finales:\")\n",
    "print(f\"Accuracy en entrenamiento (threshold={threshold}): {accuracy_train:.4f}\")\n",
    "print(f\"Accuracy en prueba (threshold={threshold}): {accuracy_test:.4f}\")\n",
    "print(f\"Porcentaje de overfitting: {overfitting_percentage:.2f}%\")\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_test, y_pred_test_thresholded))\n",
    "\n",
    "# Visualizar las características más importantes\n",
    "if hasattr(selector, 'get_feature_names_out'):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    selected_features = selector.get_support()\n",
    "    important_features = [(feature_names[i], selector.scores_[i]) \n",
    "                         for i in range(len(feature_names)) \n",
    "                         if selected_features[i]]\n",
    "    \n",
    "    important_features.sort(key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nTop 10 características más importantes:\")\n",
    "    for feature, score in important_features[:10]:\n",
    "        print(f\"{feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Crear un diccionario con todos los componentes\n",
    "model_components = {\n",
    "    'vectorizer': vectorizer,\n",
    "    'selector': selector,\n",
    "    'model': best_model,\n",
    "    'threshold': threshold\n",
    "}\n",
    "\n",
    "# Guardar el modelo\n",
    "joblib.dump(model_components, '../models/ensemble_model_complete.joblib')\n",
    "\n",
    "print(\"Modelo guardado exitosamente!\")\n",
    "\n",
    "# Para cargar y usar el modelo más tarde:\n",
    "def load_and_predict(text):\n",
    "    # Cargar el modelo\n",
    "    loaded_components = joblib.load('../models/ensemble_model_complete.joblib')\n",
    "    \n",
    "    # Extraer componentes\n",
    "    vectorizer = loaded_components['vectorizer']\n",
    "    selector = loaded_components['selector']\n",
    "    model = loaded_components['model']\n",
    "    threshold = loaded_components['threshold']\n",
    "    \n",
    "    # Procesar el texto nuevo\n",
    "    x_new_vectorized = vectorizer.transform([text])\n",
    "    x_new_reduced = selector.transform(x_new_vectorized)\n",
    "    \n",
    "    # Predecir\n",
    "    probability = model.predict_proba(x_new_reduced)[0, 1]\n",
    "    prediction = 1 if probability >= threshold else 0\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# # Ejemplo de uso:\n",
    "\n",
    "# # Cargar y usar el modelo\n",
    "# texto_ejemplo = \"Ejemplo de texto para clasificar...\"\n",
    "# prediccion, probabilidad = load_and_predict(texto_ejemplo)\n",
    "# print(f\"Predicción: {prediccion}\")\n",
    "# print(f\"Probabilidad: {probabilidad:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
